{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import importlib\n",
    "import pprint\n",
    "import json\n",
    "\n",
    "from typing import Tuple, Callable, Any, NoReturn, List, Dict, Optional, Union\n",
    "\n",
    "from functools import partial, reduce\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pdf2image.exceptions import (\n",
    "    PDFInfoNotInstalledError,\n",
    "    PDFPageCountError,\n",
    "    PDFSyntaxError\n",
    ")\n",
    "\n",
    "from pdf2image import convert_from_path, convert_from_bytes\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timing\n",
    "importlib.reload(timing)\n",
    "import timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a multiprocess pool.\n",
    "pool = mp.Pool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing.time_log()\n",
    "def images_to_strings(x) -> Dict[str, List[str]]:\n",
    "    \"\"\" Envoltura para generar un diccionario del tipo :\n",
    "            {\n",
    "                \"nombre del archivo PDF\": [Lista de strings, uno por página]\n",
    "            }\n",
    "        \n",
    "        A partir de un diccionario del tipo:\n",
    "            { \n",
    "                \"nombre del achivo PDF\": [Lista contendiendo una imagen por página]\n",
    "            }\n",
    "            \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    return {\n",
    "        nombre: pool.map(pytesseract.image_to_string, archivo) for nombre, archivo in x.items()\n",
    "    }\n",
    "##\n",
    "\n",
    "@timing.time_log()\n",
    "def tab_by_regex(x: str) -> str:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    newline = lambda x: f\"{x}\\n\"\n",
    "    ntab    = lambda n, txt: n*\"\\t\" + txt\n",
    "    \n",
    "    _x_lines: List[str] = x.split('\\n')\n",
    "    _my_str:  str       = ''\n",
    "        \n",
    "    for line in _x_lines:\n",
    "        # Match lines starting with Caps, separated by spaces or points : \n",
    "        # r\"^([A-Z\\s|[A-Z]\\.?)+?(?=(\\..+))\" # found on a separate cell down\n",
    "        # r\"^([A-Z\\s]|[A-Z]\\.?)+?(?=(\\..+))\" # the legacy regex for this func\n",
    "        for i in regex.finditer(r\"^([A-Z\\s|[A-Z]\\.?)+?(?=(\\..+))\", line): \n",
    "            _my_str += newline(i.string)\n",
    "            _my_str += newline(ntab(1, i.group()))\n",
    "            # Match numbers, which could be either integers or floats : \n",
    "            for j in regex.finditer(r\"(\\d+\\.\\d+|\\d+)\", i.string): # find groups of numbers\n",
    "                _my_str += newline(ntab(2, j.group()))\n",
    "            # Match strings found between numbers :\n",
    "            for k in regex.finditer(r\"(?<=(\\d+\\.\\d+|\\d+))\\D[^\\d]+?(?=\\s)\", i.string):\n",
    "                _my_str += newline(ntab(3, k.group()))\n",
    "    \n",
    "    return _my_str\n",
    "##\n",
    "\n",
    "@timing.time_log()\n",
    "def dict_from_regex(x: str) -> str:\n",
    "    \"\"\"\n",
    "        Genera un diccionario listo para ser serializado a JSON-Lines.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    _x_lines: List[str] = x.split('\\n')\n",
    "    \n",
    "    _dict2 = {}\n",
    "    for line in _x_lines:\n",
    "        for i in regex.finditer(r\"^([A-Z\\s|[A-Z]\\.?)+?(?=(\\..+))\", line): \n",
    "            i.group()\n",
    "            _valores = []\n",
    "            _unidades = []\n",
    "            # Match numbers, which could be either integers or floats : \n",
    "            for j in regex.finditer(r\"(\\d+\\.\\d+|\\d+)\", i.string):\n",
    "                _valores.append(float(j.group()))\n",
    "            # Match strings found between numbers :\n",
    "            for k in regex.finditer(r\"(?<=(\\d+\\.\\d+|\\d+))\\D[^\\d]+?(?=\\s)\", i.string):\n",
    "                _unidades.append(k.group())\n",
    "            if len(_valores) < 5 and len(_unidades) < 5:\n",
    "                _dict2.update({\n",
    "                    i.group(): {\n",
    "                        \"values\": _valores,\n",
    "                        \"units\": _unidades\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "            # We purge _dict2, allowing just matches having more than\n",
    "            # one value, as is makes no sense to log void entries.\n",
    "            _dict2 = {\n",
    "                key: value for key, value in _dict2.items() if len(value['values']) > 0\n",
    "            }\n",
    "    \n",
    "    return _dict2\n",
    "##\n",
    "\n",
    "@timing.time_log()\n",
    "def dict_from_regex2(x: str, date: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "        Genera un diccionario listo para ser serializado a JSON-Lines.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    _x_lines: List[str] = x.split('\\n')\n",
    "    \n",
    "    _dict2 = {}\n",
    "    for line in _x_lines:\n",
    "        for i in regex.finditer(r\"^([A-Z\\s|[A-Z]\\.?)+?(?=(\\..+))\", line): \n",
    "            i.group()\n",
    "            _valores = []\n",
    "            _unidades = []\n",
    "            # Match numbers, which could be either integers or floats : \n",
    "            for j in regex.finditer(r\"(\\d+\\.\\d+|\\d+)\", i.string):\n",
    "                _valores.append(float(j.group()))\n",
    "            # Match strings found between numbers :\n",
    "            for k in regex.finditer(r\"(?<=(\\d+\\.\\d+|\\d+))\\D[^\\d]+?(?=\\s)\", i.string):\n",
    "                _unidades.append(k.group())\n",
    "            if len(_valores) < 5 and len(_unidades) < 5:\n",
    "                _dict2.update({\n",
    "                    i.group(): {\n",
    "                        \"values\": _valores,\n",
    "                        \"units\": _unidades\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "            # We purge _dict2, allowing just matches having more than\n",
    "            # one value, as is makes no sense to log void entries.\n",
    "            _dict2 = {\n",
    "                key: value for key, value in _dict2.items() if len(value['values']) > 0\n",
    "            }\n",
    "    \n",
    "    if date is not None:\n",
    "        return { date: _dict2 }\n",
    "    else:\n",
    "        return _dict2\n",
    "##\n",
    "\n",
    "@timing.time_log()\n",
    "def save_results(x: Dict[str, List[str]]) -> bool:\n",
    "    \"\"\"\n",
    "        Guarda los strings a los cuales ya se les han aplicado los filtros.\n",
    "            \n",
    "        El modo de escritura es 'w', o sea 'write'.\n",
    "        Cada vez que se llame esta función los resultados anteriormente guardados \n",
    "        serán sobreescritos por los nuevos generados.\n",
    "        \n",
    "        La función crea un directorio llamado 'segmented' si es que \n",
    "        éste no existe.\n",
    "        \n",
    "        Después guarda todos los archivos en dicho directorio.\n",
    "        \n",
    "        Regresa:\n",
    "            'True',  si se logran generar los archivos.\n",
    "            \n",
    "            'False', si ocurre algún error.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        if 'segmented' not in os.listdir('.'):\n",
    "            os.mkdir('segmented')\n",
    "        \n",
    "        _camino_resultados = os.path.abspath('segmented')\n",
    "        _archivos =  list(x.keys())\n",
    "    \n",
    "        for i, archivo in enumerate(_archivos):\n",
    "            for j, page in enumerate(x[archivo]):\n",
    "                with open(os.path.join(_camino_resultados, f\"{_archivos[i].replace('.pdf', '')}.{j+1}.txt\"), 'w') as f:\n",
    "                    f.write(tab_by_regex(page))\n",
    "    \n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "##                                    \n",
    "\n",
    "@timing.time_log()\n",
    "def extract_dates(\n",
    "    x: Dict[str, List[str]], \n",
    "    exclude_date: Optional[str] = None\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "        Busca las fechas en un diccionario del siguiente formato :\n",
    "            { str: Dict[str, List[str]] }\n",
    "            \n",
    "        Genera un diccionario con las mismas llaves, pero con una lista \n",
    "        de las fechas que encontró en las hojas (cada uno de los strings \n",
    "        dentro de la lista).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    dates = {}\n",
    "    for nombre, lista in x.items():\n",
    "        _tmp_list = []\n",
    "        for string in lista:\n",
    "            _tmp_list += list(set(\n",
    "                regex.findall(r\"(\\d{2}\\D[A-Z]{3}\\D\\d{4}|\\d{2}\\D\\d{2}\\D\\d{4})\", string)\n",
    "            ))\n",
    "        dates.update({\n",
    "            nombre: list(set(_tmp_list))\n",
    "        })\n",
    "\n",
    "    if exclude_date is not None:\n",
    "        for nombre, lista in dates.items():\n",
    "            dates[nombre] = list(\n",
    "                filter(lambda x: x if x != exclude_date else False, lista)\n",
    "            )\n",
    "    \n",
    "    return dates\n",
    "##\n",
    "                                       \n",
    "@timing.time_log()\n",
    "def save_docs_to_jsonl(x: Dict[str, List[str]], filename: Optional[str] = None) -> bool:\n",
    "    \"\"\"\n",
    "        Guarda los resultados en un archivo, cuyo nombre puede \n",
    "        ser opcionalmente especificado a través del parámetro 'filename'.\n",
    "        \n",
    "        El NOMBRE por defecto es :\n",
    "            'resultados.jl'\n",
    "            \n",
    "        El nombre del archivo 'filename' puede ser un camino absoluto, si\n",
    "        se desea guardar la información en algún otro lugar.\n",
    "        \n",
    "        POR DEFECTO se trata del mismo directorio :\n",
    "            En la línea de comando :\n",
    "                `pwd`\n",
    "                \n",
    "            Desde Python :\n",
    "                os.path.abspath('.')\n",
    "        \n",
    "        El formato de registro es JSON-lines :\n",
    "            http://jsonlines.org/\n",
    "            \n",
    "        El modo de edición del archivo es 'append', \n",
    "        es decir los nuevos registros se añaden, sin borrar \n",
    "        las líneas anteriores.\n",
    "        \n",
    "        Prámetros :\n",
    "                   x: Diccionario { str: List[str] }, la información por guardar.\n",
    "            filename: Opcional, str, nombre del archivo de registro.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        _archivos = x.keys()\n",
    "        _dates =  extract_dates(x)\n",
    "        if filename is None:\n",
    "            filename = 'resultados.jl'\n",
    "                                       \n",
    "        with open(filename, 'a') as f:\n",
    "            for archivo in _archivos:\n",
    "                for hoja in x[archivo]:\n",
    "                    f.write(f\"{json.dumps(dict_from_regex2(hoja, _dates[archivo]))}\\n\")\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "##\n",
    "\n",
    "@timing.time_log()                                     \n",
    "def build_string_from_dict(x: Union[Dict[str, List[str]], List[Dict[str, Any]]]) -> str:\n",
    "    ''' \n",
    "        Construye una cadena de caracteres a partir de una de\n",
    "        dos posibles estructuras anidadas :\n",
    "        \n",
    "        1. Diccionario :  {   str: List[str] }\n",
    "        2. Lista       :  [ { str: Any }     ]\n",
    "        \n",
    "        Esta cadena de caracteres es óptima para la visualización.\n",
    "    '''\n",
    "    newline = lambda x: f\"{x}\\n\"\n",
    "    ntab = lambda n, txt: n*\"\\t\" + txt\n",
    "                                       \n",
    "    my_string = ''\n",
    "    \n",
    "    if type(x) is list:\n",
    "        for entry in x:\n",
    "            for key in entry.keys():\n",
    "                my_string += f'{key}: {entry[key]}\\n'\n",
    "            my_string += '\\n\\n'\n",
    "    elif type(x) is dict:\n",
    "        for key1, value in x.items():\n",
    "            my_string += f\"{key1}:\\n\"\n",
    "            for key2 in value.keys():\n",
    "                my_string += f'\\t{key2}: {value[key2]}\\n'\n",
    "            my_string += '\\n\\n'\n",
    "                                       \n",
    "    \n",
    "    return my_string\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de asistencia \"helper functions\" :\n",
    "newline = lambda x: f\"{x}\\n\"\n",
    "ntab = lambda n, txt: n*\"\\t\" + txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/gml/Documents/IX/imagenes/FinalDeImagenes/analisis_clinicos'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.abspath('analisis_clinicos/')\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí se encuentran todos los pdf con análisis clínicos que tenemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/gml/Documents/IX/imagenes/FinalDeImagenes/textos'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_textos = os.path.abspath('textos')\n",
    "path_textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta dirección guardaremos todos los textos reconocidos por **pytesseract**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminos_textos = glob.glob(f\"{path_textos}/*.txt\")\n",
    "caminos_textos.sort()\n",
    "#caminos_textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta dirección se almacenaron los textos que se extrajeron de los documentos a partir de **pytesseract**. Nótese que al procesar imagen por imagen, y al crear nosotros una imagen por cada página del pdf, los nombres que ahí se encuentran son los originales, sin la extensión pdf, con un punto indicando el nombre de página y con terminación txt.\n",
    "\n",
    "```mi_archivo.pdf```  => ```mi_archivo.i.txt``` Donde **i** indica el número de página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminos = glob.glob(f\"{path}/*.pdf\")\n",
    "#caminos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta lista contiene el camino hacia cada uno de los PDFs de los cuales se desean extraer los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gustavo_maganna_2018-05-13.pdf',\n",
       " 'gustavo_maganna_2018-05-06.pdf',\n",
       " 'gustavo_maganna_2019-11-13.pdf',\n",
       " 'Wed Nov 13 18:05:58 CST 2019.pdf',\n",
       " 'InformeResultados1-110600.pdf',\n",
       " 'gustavo_maganna_2018-01-19.pdf']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archivos = [ os.path.split(camino)[1] for camino in caminos]\n",
    "archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre de los archivos PDF, sin el camino absoluto dentro del *filesystem*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gustavo_maganna_2018-05-13',\n",
       " 'gustavo_maganna_2018-05-06',\n",
       " 'gustavo_maganna_2019-11-13',\n",
       " 'Wed Nov 13 18:05:58 CST 2019',\n",
       " 'InformeResultados1-110600',\n",
       " 'gustavo_maganna_2018-01-19']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nombres = [ archivo.replace('.pdf', '') for archivo in archivos ]\n",
    "nombres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre de los archivos, sin la extensión ```.pdf```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_from_txt = True\n",
    "\n",
    "if parse_from_txt:\n",
    "    # Buscamos todos los archivos de texto :\n",
    "    #   los nombres de archivo especificados por 'nombres'\n",
    "    #   en el directorio especificado por 'path_textos'\n",
    "    archivos_de_texto = { \n",
    "        archivo: glob.glob(os.path.join(path_textos ,f\"{nombre}.?.txt\")) \n",
    "        for archivo, nombre in zip(archivos, nombres)\n",
    "    }\n",
    "    # Los ordenamos, para que el orden de las páginas sea el mismo \n",
    "    # que en los PDFs :\n",
    "    [ archivos_de_texto[key].sort() for key in archivos_de_texto.keys() ]\n",
    "    \n",
    "    # Creamos un diccionario vacío el cual contendrá todas las cadenas \n",
    "    # de acaracteres asociadas a cada una de las hojas de cada uno de los reportes\n",
    "    # de análisis clínicos.\n",
    "    strings = {}\n",
    "    \n",
    "    # Iteramos sobre cada uno de los nombres de archivo pdf\n",
    "    for key in archivos_de_texto.keys():\n",
    "        # Generamos una lista vacía para contener las 'n'\n",
    "        # cadenas de caracteres, correspondientes a las 'n'\n",
    "        # páginas de cada pdf.\n",
    "        _strings = []\n",
    "        for _file in archivos_de_texto[key]:\n",
    "            # Iteramos sobre cada 'hoja' perteneciente al pdf especificado.\n",
    "            with open(_file, 'r') as f:\n",
    "              _strings.append(f.read())\n",
    "        strings.update({\n",
    "            # Añadimos el archivo como llave y \n",
    "            # la lista de strings como valor al diccionario.\n",
    "            key: _strings\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain text from the PDFs, directly (this takes about a minute) : \n",
    "from_pdfs = False\n",
    "\n",
    "if from_pdfs:\n",
    "    # Para acelerar el proceso, generamos imágenes de forma paralela\n",
    "    # gracias a pool.map :\n",
    "    imagenes = pool.map(convert_from_path, caminos)\n",
    "    # Construimos un diccionario con los nombres de archivos y las\n",
    "    # imágenes obtenidas : \n",
    "    archivos_en_imagenes = {\n",
    "        archivo: imagen for archivo, imagen in zip(archivos, imagenes)\n",
    "    }\n",
    "    # Generamos un nuevo diccionario, ahora con cadenas de caracteres\n",
    "    # en lugar de imágenes, estas primeras obtenidas a través de \n",
    "    # pytesseract.\n",
    "    # la función images_to_strings() utiliza a su vez pool.map, para\n",
    "    # acelerar el proceso de extracción de caracteres.\n",
    "    strings = images_to_strings(archivos_en_imagenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos cada uno de los strings generados, \n",
    "# para poder posteriormente acceder a ellos sin necesidad de \n",
    "# repetir el procesamiento de imágenes, lo que conforme \n",
    "# crezca la base de datos del PDF, se hará más lento.\n",
    "save = True\n",
    "\n",
    "if save:\n",
    "    for nombre, hojas in strings.items():\n",
    "        for i, hoja in enumerate(hojas):\n",
    "            _file_name = os.path.join(path_textos, f\"{nombre.replace('.pdf', '')}.{i+1}.txt\")\n",
    "            with open(_file_name, \"w\") as f:\n",
    "                f.write(hoja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gustavo_maganna_2018-05-13.pdf': ['13/05/2018'],\n",
       " 'gustavo_maganna_2018-05-06.pdf': [],\n",
       " 'gustavo_maganna_2019-11-13.pdf': ['13-NOV-2019'],\n",
       " 'Wed Nov 13 18:05:58 CST 2019.pdf': ['13-NOV-2019'],\n",
       " 'InformeResultados1-110600.pdf': ['15-NOV-2019'],\n",
       " 'gustavo_maganna_2018-01-19.pdf': ['19-ENE-2018']}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_dates(strings, exclude_date='06/08/1996')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función que creamos ```extract_date()``` permite obtener fechas encontradas en un PDF, con flexibilidad en cuanto a formatos y la posibilidad de excluir una fecha especificada, que bien podría ser la fecha de cumpleaños del paciente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "example0 = False\n",
    "\n",
    "if example0:\n",
    "    print(tab_by_regex(strings[archivos[1]][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí observamos cómo la función que definimos ```tab_by_regex()``` puede identficar correctamente los parámetros de interés, aunque incluye datos que pueden no ser los deseados ya que el reconocimiento de caracteres de **pytesseract** no es perfecto.\n",
    "\n",
    "Esto se deberá tomar en cuenta el momento de generar los registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPURACION DE CREATININA.:\n",
      "\tvalues: [133.6, 75.0, 115.0]\n",
      "\tunits: [' mL/min']\n",
      "\n",
      "\n",
      "CREATININA.:\n",
      "\tvalues: [0.92, 0.61, 1.24]\n",
      "\tunits: [' mg/dL', ' -']\n",
      "\n",
      "\n",
      "CREATININA URINARIA:\n",
      "\tvalues: [82.6, 39.0, 259.0]\n",
      "\tunits: [' mg/dL', ' -']\n",
      "\n",
      "\n",
      "SUP:\n",
      "\tvalues: [1.86, 2.0]\n",
      "\tunits: []\n",
      "\n",
      "\n",
      "MICROALBUMINURIA.:\n",
      "\tvalues: [10.0, 0.0, 100.0]\n",
      "\tunits: [' mg/L']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example1 = True\n",
    "\n",
    "if example1:\n",
    "    ejemplo = dict_from_regex(strings['gustavo_maganna_2018-01-19.pdf'][0])\n",
    "    print(build_string_from_dict(ejemplo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí pueden observarse los datos identificados de la primer hoja del pdf : 'gustavo_maganna_2018-01-19.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "example2 = False\n",
    "\n",
    "if example2:\n",
    "    _file = archivos[1]\n",
    "    print(f\" Nombre del archivo: {_file}\")\n",
    "    print(f\" Número de hojas: {len(strings[_file])}\")\n",
    "    _pagenum = 2\n",
    "    print(f\" Contenido de la hoja {_pagenum + 1}: \\n\\n {strings[_file][_pagenum]}\")\n",
    "    print(\"\\n\\n######################################################################\\n\\n\")\n",
    "    print(\"Parámetros encontrados : \\n\",build_string_from_dict(dict_from_regex(strings[_file][_pagenum]) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este ejemplo permite contrastar el texto identificado por **pytesseract** y los valores obtenidos una vez que se ha aplicado nuestro sistema de filtro para encontrar los parámetros de interés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_docs_to_jsonl(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí hemos guardado los registros en un archivo JSON-Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_results(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí hemos guardado el texto extraido de las imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "example3 = False\n",
    "\n",
    "if example3:\n",
    "    for archivo in archivos:\n",
    "        for hoja in strings[archivo]:\n",
    "            print(build_string_from_dict(dict_from_regex(hoja)))\n",
    "            #print(tab_by_regex(hoja))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My regexps :\n",
    "\n",
    "my_may_ge_2 = r\"^[A-Z]{2,}\"  # Find CAPITAL WORDS longer than 2, at the begining of the line.\n",
    "my_numbers = r\"(\\d+\\.\\d+|\\d+)\" # Find numbers of any length, with a decimal point or not.\n",
    "my_from_begining_until_point = r\"^.+?(?=(\\..+))\" # Find any text before the first occurrence of a pint\n",
    "my_caps_and_whitespace_until_point = r\"^[A-Z\\s]+?(?=(\\..+))\"\n",
    "my_caps_and_whitspace_or_acronym_until_point = r\"^([A-Z\\s|[A-Z]\\.?)+?(?=(\\..+))\"\n",
    "my_failed_extract_units = r\"(?<=((\\d+\\.\\d+|\\d+)+))(.+)\"\n",
    "googled_text_between_brackets = r\"(?<=\\[).+?(?=\\])\"\n",
    "my_extract_units = r\"(?<=(\\d+\\.\\d+|\\d+))\\D+?(?=(\\d+\\.\\d+|\\d+))\"\n",
    "my_better_extract_units_between_numbers = r\"(?<=(\\d+\\.\\d+|\\d+))\\D[^\\.\\d]+?(?=(\\d+\\.\\d+|\\d+))\"\n",
    "my_extract_units_between_numbers_and_whitespace = r\"(?<=(\\d+\\.\\d+|\\d+))\\D[^\\.\\d]+?(?=\\s)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta celda podemos encontrar las expresiones regulares que fueron utilizadas en el proceso de exploración para así identificar las óptimas para poder extraer los datos de las hojas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuántas líneas tenemos en total :\n",
    "caracteres_por_linea = pd.core.series.Series(\n",
    "    [\n",
    "     len(line) for line in page.split('\\n') \n",
    "     for pdf in strings\n",
    "     for page in pdf\n",
    "    ],\n",
    "    name='caracteres'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a242c5750>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(caracteres_por_linea, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.close()\n",
    "pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
